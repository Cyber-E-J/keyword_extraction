{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 15:17:21 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-11-07 15:17:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-11-07 15:17:21 INFO: Use device: cpu\n",
      "2022-11-07 15:17:21 INFO: Loading: tokenize\n",
      "2022-11-07 15:17:21 INFO: Loading: pos\n",
      "2022-11-07 15:17:21 INFO: Loading: lemma\n",
      "2022-11-07 15:17:21 INFO: Loading: depparse\n",
      "2022-11-07 15:17:21 INFO: Loading: ner\n",
      "2022-11-07 15:17:22 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,ner' ,download_method = None)\n",
    "\n",
    "print(\"import success\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_file(filename):\n",
    "    file = open(filename,\"r\")\n",
    "    lines = file.readlines()\n",
    "    dialogues = []\n",
    "    for line in lines:\n",
    "        dialogue_in = line.find(\"dialogue\")\n",
    "        if (dialogue_in!=-1):\n",
    "            dialogues.append(line.strip()[13:-2])\n",
    "    return dialogues\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop_words = ['file_gif','file_other','\\'s', '\\'m', 'file_photo', 'u' , 'urgh', 'r' , 'haha' ,\n",
    "                  'ok', 'eh' , '\\'re', 'yeah', 'file_video' ,'oh' , 'xd' , 'yes' , 'bye' , 'yup',\n",
    "                  'lol' , 'be' , 'hey', 'sure' ,  'yep' , 'know', 'really' , 'sorry', 'that', 'wtf',\n",
    "                  'get', 'go' , ''\n",
    "                 ]\n",
    "for word in new_stop_words:\n",
    "    stop_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1694\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/erlich_jaso/Code/stanza/stanza/models/common/data.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:865\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[0;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_AND_BASE_CONTAINER[frame\u001b[39m.\u001b[39;49mf_code\u001b[39m.\u001b[39;49mco_filename]\n\u001b[1;32m    866\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     \u001b[39m# This one is just internal (so, does not need any kind of client-server translation)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/erlich_jaso/Code/stanza/stanza/models/common/data.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:829\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[0;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_AND_BASE_CONTAINER[filename]\n\u001b[1;32m    830\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/erlich_jaso/Code/stanza/stanza/models/common/data.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:390\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[0;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m NORM_PATHS_CONTAINER[filename]\n\u001b[1;32m    391\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '/Users/erlich_jaso/Code/stanza/stanza/models/common/data.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/erlich_jaso/Code/parsing/extract.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         output\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m dialogue \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMark: So, we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve got our where and when. Package tour or self-organised?\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnna: Package. More convenient.\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGeorge: Self-organised. Cheaper.\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mJulia: Do we need a 5* hotel? MAybe let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms choose one of the cheaper options from a tour operator?\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMark: Actually, not a bad idea. That\u001b[39m\u001b[39m'\u001b[39m\u001b[39mll be both cheap and convenient.\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnna: I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm in!\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGeorge: So, let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms start digging and we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mll talk about it l8r?\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMark: SLAP\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mJulia: Ok. But maybe let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms divide ourselves so that we don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt check the same websites?\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGeorge: Ur right!\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnna: Sure. XOXOXOX\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMark: Let\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms do this asap!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m get_keyword_from_dialogue(dialogue)\n",
      "\u001b[1;32m/Users/erlich_jaso/Code/parsing/extract.ipynb Cell 3\u001b[0m in \u001b[0;36mget_keyword_from_dialogue\u001b[0;34m(dialogue)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m start_pos \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m sentence \u001b[39m=\u001b[39m sentence[start_pos\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m get_keyword_from_sentence(sentence):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     keywords\u001b[39m.\u001b[39mappend(word)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m clean_repeat_keywords \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(keywords))\n",
      "\u001b[1;32m/Users/erlich_jaso/Code/parsing/extract.ipynb Cell 3\u001b[0m in \u001b[0;36mget_keyword_from_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_keyword_from_sentence\u001b[39m(sentence):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     keywords \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msentences:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/erlich_jaso/Code/parsing/extract.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         entities \u001b[39m=\u001b[39m sent\u001b[39m.\u001b[39mentities\n",
      "File \u001b[0;32m~/Code/stanza/stanza/pipeline/core.py:408\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[0;32m~/Code/stanza/stanza/pipeline/core.py:397\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[1;32m    396\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[0;32m--> 397\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[1;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Code/stanza/stanza/pipeline/pos_processor.py:67\u001b[0m, in \u001b[0;36mPOSProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, document):\n\u001b[0;32m---> 67\u001b[0m     batch \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     68\u001b[0m         document, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrain, vocab\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab, evaluation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     69\u001b[0m         sort_during_eval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     70\u001b[0m     preds \u001b[39m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tqdm:\n",
      "File \u001b[0;32m~/Code/stanza/stanza/models/pos/data.py:48\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, doc, batch_size, args, pretrain, vocab, evaluation, sort_during_eval)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     47\u001b[0m \u001b[39m# chunk into batches\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_batches(data)\n\u001b[1;32m     49\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m batches created.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)))\n",
      "File \u001b[0;32m~/Code/stanza/stanza/models/pos/data.py:154\u001b[0m, in \u001b[0;36mDataLoader.chunk_batches\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(data, key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x[\u001b[39m0\u001b[39m]), reverse\u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mrandom() \u001b[39m>\u001b[39m \u001b[39m.5\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_during_eval:\n\u001b[0;32m--> 154\u001b[0m     (data, ), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_orig_idx \u001b[39m=\u001b[39m sort_all([data], [\u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m data])\n\u001b[1;32m    156\u001b[0m current \u001b[39m=\u001b[39m []\n\u001b[1;32m    157\u001b[0m currentlen \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Code/stanza/stanza/models/common/data.py:41\u001b[0m, in \u001b[0;36msort_all\u001b[0;34m(batch, lens)\u001b[0m\n\u001b[1;32m     38\u001b[0m         features[i,:\u001b[39mlen\u001b[39m(f),:] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(f)\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m features\n\u001b[0;32m---> 41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msort_all\u001b[39m(batch, lens):\n\u001b[1;32m     42\u001b[0m     \u001b[39m\"\"\" Sort all fields by descending order of lens, and return the original indices. \"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m batch \u001b[39m==\u001b[39m [[]]:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1696\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:883\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[0;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    880\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(f\u001b[39m.\u001b[39mrfind(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m), f\u001b[39m.\u001b[39mrfind(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m f, f, f[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 883\u001b[0m ret \u001b[39m=\u001b[39m get_abs_path_real_path_and_base_from_file(f)\n\u001b[1;32m    884\u001b[0m \u001b[39m# Also cache based on the frame.f_code.co_filename (if we had it inside build/bdist it can make a difference).\u001b[39;00m\n\u001b[1;32m    885\u001b[0m NORM_PATHS_AND_BASE_CONTAINER[frame\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_filename] \u001b[39m=\u001b[39m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:850\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[0;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[39melif\u001b[39;00m f\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m$py.class\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    848\u001b[0m         f \u001b[39m=\u001b[39m f[:\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m$py.class\u001b[39m\u001b[39m'\u001b[39m)] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 850\u001b[0m abs_path, canonical_normalized_filename \u001b[39m=\u001b[39m _abs_and_canonical_path(f)\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    853\u001b[0m     base \u001b[39m=\u001b[39m os_path_basename(canonical_normalized_filename)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:413\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[0;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[1;32m    410\u001b[0m abs_path \u001b[39m=\u001b[39m _apply_func_and_normalize_case(filename, os_path_abspath, isabs, normalize)\n\u001b[1;32m    412\u001b[0m normalize \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m real_path \u001b[39m=\u001b[39m _apply_func_and_normalize_case(filename, os_path_real_path, isabs, normalize)\n\u001b[1;32m    415\u001b[0m \u001b[39m# cache it for fast access later\u001b[39;00m\n\u001b[1;32m    416\u001b[0m NORM_PATHS_CONTAINER[filename] \u001b[39m=\u001b[39m abs_path, real_path\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd_file_utils.py:440\u001b[0m, in \u001b[0;36m_apply_func_and_normalize_case\u001b[0;34m(filename, func, isabs, normalize_case, os_path_exists, join)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Not really a file, rather a synthetic name like <string> or <ipython-...>;\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39m# shouldn't be normalized.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[0;32m--> 440\u001b[0m r \u001b[39m=\u001b[39m func(filename)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isabs:\n\u001b[1;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os_path_exists(r):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/posixpath.py:392\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[39m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 392\u001b[0m     path, ok \u001b[39m=\u001b[39m _joinrealpath(filename[:\u001b[39m0\u001b[39;49m], filename, {})\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/posixpath.py:425\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, seen)\u001b[0m\n\u001b[1;32m    423\u001b[0m         path \u001b[39m=\u001b[39m pardir\n\u001b[1;32m    424\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m newpath \u001b[39m=\u001b[39m join(path, name)\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m islink(newpath):\n\u001b[1;32m    427\u001b[0m     path \u001b[39m=\u001b[39m newpath\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/posixpath.py:88\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     86\u001b[0m             path \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m b\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m             path \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sep \u001b[39m+\u001b[39m b\n\u001b[1;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m, \u001b[39mBytesWarning\u001b[39;00m):\n\u001b[1;32m     90\u001b[0m     genericpath\u001b[39m.\u001b[39m_check_arg_types(\u001b[39m'\u001b[39m\u001b[39mjoin\u001b[39m\u001b[39m'\u001b[39m, a, \u001b[39m*\u001b[39mp)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "key_deprel = ['root', 'csubj', 'nsubj','xsubj', 'cop', 'vmod','dobj' ,'iobj', 'pobj']\n",
    "def get_keyword_from_sentence(sentence):\n",
    "    keywords = []\n",
    "    doc = nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        entities = sent.entities\n",
    "        entities_name = [entity.text for entity in entities]\n",
    "\n",
    "\n",
    "        for word in sent.words:\n",
    "            if(\n",
    "               (word.deprel in key_deprel  # select word with parsing\n",
    "                and word.text.lower() not in stop_words # delete stopword\n",
    "               )\n",
    "\n",
    "               or \n",
    "               \n",
    "               word.text.lower() in entities_name # word is an entity\n",
    "               ):\n",
    "                # keywords.append(word.text.lower())\n",
    "                keywords.append(word.lemma.lower())\n",
    "    return keywords\n",
    "\n",
    "\n",
    "\n",
    "def get_keyword_from_dialogue(dialogue):\n",
    "    keywords = []\n",
    "    for sentence in dialogue:\n",
    "        start_pos = sentence.find(\":\")\n",
    "        sentence = sentence[start_pos+2:]\n",
    "        for word in get_keyword_from_sentence(sentence):\n",
    "            keywords.append(word)\n",
    "        clean_repeat_keywords = list(set(keywords))\n",
    "    return clean_repeat_keywords\n",
    "\n",
    "def get_keyphrase_from_file(file):\n",
    "    dialogues = []\n",
    "    for dialogue in file:\n",
    "        if(dialogue.find('\\\\r\\\\n')>0 ):\n",
    "            sentences = dialogue.split('\\\\r\\\\n')\n",
    "        else: \n",
    "            sentences = dialogue.split('\\\\n')\n",
    "        dialogues.append(sentences)\n",
    "\n",
    "    keyphrase = []\n",
    "    for dialogue in dialogues:\n",
    "        keyphrase.append(get_keyword_from_dialogue(dialogue))\n",
    "    return keyphrase\n",
    "\n",
    "def get_keyphrase(filename):\n",
    "    file = open_file(filename)\n",
    "    length = len(file)\n",
    "    # length = 20\n",
    "    print(\"file length = {}\".format(length))\n",
    "\n",
    "\n",
    "    keyphrase = []\n",
    "    for i in range(0, length, 10):\n",
    "        output_2_txt(output, get_keyphrase_from_file(file[i:i+10]))\n",
    "        print(\"dialogue {} to {} parsed\".format(i,i+9))\n",
    "\n",
    "    return keyphrase\n",
    "\n",
    "def output_2_txt( output, file):\n",
    "    for line in file:\n",
    "        for word in line:\n",
    "            output.write(word+' ')\n",
    "        output.write('\\n')\n",
    "\n",
    "dialogue = \"Mark: So, we've got our where and when. Package tour or self-organised?\\r\\nAnna: Package. More convenient.\\r\\nGeorge: Self-organised. Cheaper.\\r\\nJulia: Do we need a 5* hotel? MAybe let's choose one of the cheaper options from a tour operator?\\r\\nMark: Actually, not a bad idea. That'll be both cheap and convenient.\\r\\nAnna: I'm in!\\r\\nGeorge: So, let's start digging and we'll talk about it l8r?\\r\\nMark: SLAP\\r\\nJulia: Ok. But maybe let's divide ourselves so that we don't check the same websites?\\r\\nGeorge: Ur right!\\r\\nAnna: Sure. XOXOXOX\\r\\nMark: Let's do this asap!\"\n",
    "\n",
    "get_keyword_from_dialogue(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_stop_words = ['file_gif','file_other','\\'s', '\\'m', 'file_photo', 'u' , 'urgh', 'r' , 'haha' ,\n",
    "                  'ok', 'eh' , '\\'re', 'yeah', 'file_video' ,'oh' , 'xd' , 'yes' , 'bye' , 'yup',\n",
    "                  'lol' , 'be' , 'hey', 'sure' ,  'yep' , 'know', 'really' , 'sorry', 'that', 'wtf',\n",
    "                  'get', 'go' , ''\n",
    "                 ]\n",
    "for word in new_stop_words:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eh',\n",
       " 'few',\n",
       " 'file_gif',\n",
       " 'file_other',\n",
       " 'file_photo',\n",
       " 'file_video',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'get',\n",
       " 'go',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'haha',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hey',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'know',\n",
       " 'll',\n",
       " 'lol',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'r',\n",
       " 're',\n",
       " 'really',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'sorry',\n",
       " 'such',\n",
       " 'sure',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'urgh',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wtf',\n",
       " 'xd',\n",
       " 'y',\n",
       " 'yeah',\n",
       " 'yep',\n",
       " 'yes',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'yup'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab34429e0a3bced58f8d8096e4933ac78bea0360b4c97ff6b772176bd5e9ef6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
